{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93d3cae2-b44b-425d-8ee2-18b7590fc2df",
   "metadata": {},
   "source": [
    "# GLCM Feature Extraction\n",
    "The Gray Level Co-occurrence Matrix (GLCM) is a statistical method used in image processing to analyze the spatial distribution of pixel intensities in an image. It is particularly useful for texture analysis and is widely applied in fields such as remote sensing, medical imaging, and computer vision.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16aa4a03-4e5b-4907-b6e3-bd15fc0bf519",
   "metadata": {},
   "source": [
    "This function calculates a specified feature from a Gray Level Co-occurrence Matrix (GLCM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f26e85c4-aa92-4688-925e-af129da8ecc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def glcm_feature(matrix_coocurrence, featureName):\n",
    "    feature = graycoprops(matrix_coocurrence, featureName)\n",
    "    result = np.average(feature)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6804c9aa-a7ee-4f97-bb64-271e310d132a",
   "metadata": {},
   "source": [
    "## Image Preprocessing Function\n",
    "\n",
    "This function preprocesses an input image for further analysis, specifically for contour detection and ROI extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67a25606-f5fe-4d73-a47f-89ed4a76faf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "width=400\n",
    "height=400\n",
    "def preprocessingImage(image):\n",
    "    # Convert the image from BGR to RGB\n",
    "    test_img = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Convert the RGB image to grayscale\n",
    "    test_img_gray = cv2.cvtColor(test_img, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    # Apply adaptive thresholding to create a binary image\n",
    "    test_img_thresh = cv2.adaptiveThreshold(test_img_gray, 255, \n",
    "        cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, 11, 3)\n",
    "    \n",
    "    # Find contours in the thresholded image\n",
    "    cnts = cv2.findContours(test_img_thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cnts = cnts[0] if len(cnts) == 2 else cnts[1]\n",
    "    \n",
    "    # Sort contours by area in descending order\n",
    "    cnts = sorted(cnts, key=cv2.contourArea, reverse=True)\n",
    "    \n",
    "    # Extract the largest contour as the Region of Interest (ROI)\n",
    "    for c in cnts:\n",
    "        x, y, w, h = cv2.boundingRect(c)\n",
    "        test_img_ROI = test_img[y:y+h, x:x+w]\n",
    "        break\n",
    "    \n",
    "    # Resize the ROI to a fixed size\n",
    "    test_img_ROI_resize = cv2.resize(test_img_ROI, (width, height))\n",
    "    \n",
    "    # Convert the resized ROI to grayscale\n",
    "    test_img_ROI_resize_gray = cv2.cvtColor(test_img_ROI_resize, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    return test_img_ROI_resize_gray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0c8541-4eea-47aa-8786-2dc1a778abe2",
   "metadata": {},
   "source": [
    "\n",
    "## Feature Extraction and Labeling for Glaucoma Detection\n",
    "\n",
    "In our study of glaucoma detection, we extracted four key features from the images using the Gray Level Co-occurrence Matrix (GLCM). These features are chosen based on their effectiveness in characterizing texture, which is crucial for differentiating between healthy and glaucomatous eyes.\n",
    "\n",
    "## Extracted Features\n",
    "\n",
    "1. **Contrast**:\n",
    "   - Measures the intensity contrast between a pixel and its neighbor over the whole image.\n",
    "   - A higher contrast value indicates a greater difference between pixel values, suggesting more texture.\n",
    "\n",
    "2. **Homogeneity**:\n",
    "   - Indicates the closeness of the distribution of elements in the GLCM to the GLCM diagonal.\n",
    "   - Higher homogeneity values imply that the pixel values are similar, which can signify less texture complexity.\n",
    "\n",
    "3. **Energy**:\n",
    "   - Represents the uniformity of the GLCM and is calculated as the sum of squared elements in the matrix.\n",
    "   - High energy values suggest a more uniform texture.\n",
    "\n",
    "4. **Correlation**:\n",
    "   - Measures how correlated a pixel is to its neighbor in the GLCM.\n",
    "   - A higher correlation value indicates a more consistent pixel intensity pattern, which may be associated with certain textures.\n",
    "\n",
    "## Labeling\n",
    "\n",
    "- **Labeling of Images**:\n",
    "   - In our dataset, the labels are assigned as follows:\n",
    "     - **Normal images**: labeled as `0`\n",
    "     - **Glaucoma images**: labeled as `1`\n",
    "\n",
    "### Rationale for Selection:\n",
    "- **Relevance**: The chosen features have been widely recognized in literature for their effectiveness in analyzing medical images, particularly in detecting conditions like glaucoma.\n",
    "- **Computational Efficiency**: By focusing on a smaller set of features, the model can operate more efficiently, reducing computational costs and improving the speed of analysis without sacrificing performance.\n",
    "- **Interpretability**: These features offer better interpretability, allowing clinicians to understand the significance of the model's predictions based on texture analysis.\n",
    "\n",
    "In summary, these four features—contrast, homogeneity, energy, and correlation—were selected for their relevance to glaucoma detection and their ability to capture essential textural information from retinal images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29b64d00-4220-4976-9f0c-dfba3f93e54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define image dimensions and GLCM parameters\n",
    "width, height = 400, 400\n",
    "distance = 10  # Distance for GLCM\n",
    "teta = 90  # Angle for GLCM\n",
    "\n",
    "# Initialize a numpy array to store features and labels\n",
    "data_eye = np.zeros((5, 4000))  # 5 features for 5000 images\n",
    "count = 0\n",
    "indextable = ['contrast', 'homogeneity', 'energy', 'correlation', 'Label']\n",
    "\n",
    "# Dataset paths for normal and glaucoma images\n",
    "normal_dataset_path = 'Combination2/Normal/'\n",
    "glaucoma_dataset_path = 'Combination2/Glaucoma/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c610564-f67c-4079-ba49-b2e3c474df9e",
   "metadata": {},
   "source": [
    "## Feature Extraction for Normal Eye Images\n",
    "\n",
    "This section extracts texture features from normal eye images using the Gray Level Co-occurrence Matrix (GLCM). The extracted features include contrast, homogeneity, energy, and correlation, which are crucial for characterizing the texture of the images, aiding in glaucoma detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc25df15-8f33-4e81-9cad-9e67bc3f6f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2 as cv2\n",
    "from skimage.feature import graycomatrix, graycoprops#Fixed typo: changed greycomatrix to greycomatrix\n",
    "\n",
    "# List all files in the normal dataset directory\n",
    "allfiles = os.listdir(normal_dataset_path)\n",
    "\n",
    "# Initialize feature lists and label\n",
    "for file in allfiles:\n",
    "    contrast = []\n",
    "    homogeneity = []\n",
    "    energy = []\n",
    "    correlation = []\n",
    "    label = 0  # Label for normal images\n",
    "\n",
    "    # Load and preprocess the image\n",
    "    image = cv2.imread(normal_dataset_path + str(file))\n",
    "    img = preprocessingImage(image)\n",
    "\n",
    "    # Calculate the GLCM\n",
    "    glcm = graycomatrix(img, [distance], [teta], levels=256, symmetric=True, normed=True)\n",
    "\n",
    "    # Extract features from GLCM\n",
    "    contrast.append(glcm_feature(glcm, 'contrast'))\n",
    "    homogeneity.append(glcm_feature(glcm, 'homogeneity'))\n",
    "    energy.append(glcm_feature(glcm, 'energy'))\n",
    "    correlation.append(glcm_feature(glcm, 'correlation'))\n",
    "\n",
    "    # Store extracted features and label in the data array\n",
    "    data_eye[0, count] = contrast[0]\n",
    "    data_eye[1, count] = homogeneity[0]\n",
    "    data_eye[2, count] = energy[0]\n",
    "    data_eye[3, count] = correlation[0]\n",
    "    data_eye[4, count] = label\n",
    "\n",
    "    # Increment the count\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5522c434-e4cc-466f-bdc6-0b35573eb1ad",
   "metadata": {},
   "source": [
    "## `data_eye` Array Structure\n",
    "\n",
    "The `data_eye` array is used to store extracted texture features and their corresponding labels for eye images. The array is structured as follows:\n",
    "\n",
    "- **Shape**: `data_eye` is a 2D array with dimensions `(5, N)`, where `N` is the total number of images processed.\n",
    "\n",
    "- **Row Descriptions**:\n",
    "  1. **Row 0**: Contrast values extracted from the GLCM.\n",
    "  2. **Row 1**: Homogeneity values extracted from the GLCM.\n",
    "  3. **Row 2**: Energy values extracted from the GLCM.\n",
    "  4. **Row 3**: Correlation values extracted from the GLCM.\n",
    "  5. **Row 4**: Labels (0 for normal images).\n",
    "\n",
    "\n",
    "- **Usage**: \n",
    "  - Each column in the array corresponds to a single eye image, with features and labels stored sequentially.\n",
    "  - This structured format facilitates further analysis and model training for glaucoma detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89bb36c6-cb01-487a-ad8c-947669acae1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.29885616e+03, 1.43839006e+03, 1.42922065e+03, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [4.22618168e-01, 4.48600292e-01, 4.41891600e-01, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [1.12158366e-01, 1.21158411e-01, 1.24277141e-01, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [9.50896330e-01, 9.48069288e-01, 9.49321727e-01, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_eye"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e33bb19-2c96-4d04-b265-b2ec14855b32",
   "metadata": {},
   "source": [
    "## Feature Extraction for Glaucoma Eye Images\n",
    "\n",
    "This code snippet extracts texture features from glaucoma eye images using the Gray Level Co-occurrence Matrix (GLCM) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74e67e26-3896-4999-8e69-a1859d95f3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all files in the glaucoma dataset directory\n",
    "allfiles = os.listdir(glaucoma_dataset_path)\n",
    "\n",
    "# Initialize feature lists and label for glaucoma images\n",
    "for file in allfiles:\n",
    "    contrast = []  # List to store contrast values\n",
    "    homogeneity = []  # List to store homogeneity values\n",
    "    energy = []  # List to store energy values\n",
    "    correlation = []  # List to store correlation values\n",
    "    label = 1  # Label for glaucoma images\n",
    "\n",
    "    # Load and preprocess the image\n",
    "    image = cv2.imread(glaucoma_dataset_path + str(file))\n",
    "    img = preprocessingImage(image)\n",
    "\n",
    "    # Calculate the GLCM\n",
    "    glcm = graycomatrix(img, [distance], [teta], levels=256, symmetric=True, normed=True)\n",
    "\n",
    "    # Extract features from GLCM\n",
    "    contrast.append(glcm_feature(glcm, 'contrast'))\n",
    "    homogeneity.append(glcm_feature(glcm, 'homogeneity'))\n",
    "    energy.append(glcm_feature(glcm, 'energy'))\n",
    "    correlation.append(glcm_feature(glcm, 'correlation'))\n",
    "\n",
    "    # Store extracted features and label in the data array\n",
    "    data_eye[0, count] = contrast[0]\n",
    "    data_eye[1, count] = homogeneity[0]\n",
    "    data_eye[2, count] = energy[0]\n",
    "    data_eye[3, count] = correlation[0]\n",
    "    data_eye[4, count] = label\n",
    "\n",
    "    # Increment the count for the next image\n",
    "    count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9905959c-9e33-4637-ac62-eacc9fedd6ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.29885616e+03, 1.43839006e+03, 1.42922065e+03, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [4.22618168e-01, 4.48600292e-01, 4.41891600e-01, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [1.12158366e-01, 1.21158411e-01, 1.24277141e-01, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [9.50896330e-01, 9.48069288e-01, 9.49321727e-01, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_eye"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd741bc-3491-4ca3-a431-88c14ea2cfa3",
   "metadata": {},
   "source": [
    "## Creating a Pandas DataFrame from Extracted Features\n",
    "\n",
    "This code snippet initializes a Pandas DataFrame using the extracted texture features stored in the `data_eye` array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46812c1f-1f56-432c-b4fc-8679a4e87a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # Import the pandas library for data manipulation\n",
    "\n",
    "# Create a DataFrame from the transposed data_eye array, with columns specified by indextable\n",
    "df = pd.DataFrame(np.transpose(data_eye), columns=indextable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a38151-01ae-4378-8dd3-b084bc93164c",
   "metadata": {},
   "source": [
    "## Generating Descriptive Statistics for the DataFrame\n",
    "\n",
    "The `df.describe()` method provides a summary of statistics for the DataFrame `df`, which contains the extracted texture features from the `data_eye` array. This method is particularly useful for understanding the distribution and central tendencies of the features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78144b34-12f5-4462-bfef-fb8d9c5adb73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contrast</th>\n",
       "      <th>homogeneity</th>\n",
       "      <th>energy</th>\n",
       "      <th>correlation</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4000.000000</td>\n",
       "      <td>4000.000000</td>\n",
       "      <td>4000.000000</td>\n",
       "      <td>4000.000000</td>\n",
       "      <td>4000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>283.397271</td>\n",
       "      <td>0.154877</td>\n",
       "      <td>0.052399</td>\n",
       "      <td>0.406527</td>\n",
       "      <td>0.133500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>473.824334</td>\n",
       "      <td>0.185359</td>\n",
       "      <td>0.067782</td>\n",
       "      <td>0.459581</td>\n",
       "      <td>0.340157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>326.797902</td>\n",
       "      <td>0.308051</td>\n",
       "      <td>0.101121</td>\n",
       "      <td>0.920349</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1754.074124</td>\n",
       "      <td>0.885988</td>\n",
       "      <td>0.420671</td>\n",
       "      <td>0.998060</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          contrast  homogeneity       energy  correlation        Label\n",
       "count  4000.000000  4000.000000  4000.000000  4000.000000  4000.000000\n",
       "mean    283.397271     0.154877     0.052399     0.406527     0.133500\n",
       "std     473.824334     0.185359     0.067782     0.459581     0.340157\n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000\n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000\n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000\n",
       "75%     326.797902     0.308051     0.101121     0.920349     0.000000\n",
       "max    1754.074124     0.885988     0.420671     0.998060     1.000000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate and display descriptive statistics for the DataFrame\n",
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257a2d3d-a8ce-4b46-b824-dcb86eafb038",
   "metadata": {},
   "source": [
    "## Scaling the Features with MinMaxScaler\n",
    "\n",
    "The `MinMaxScaler()` is used to scale the features of the DataFrame `df` to a range between 0 and 1. The DataFrame contains various features, and the label column is excluded from scaling. This scaling process ensures that the model treats all features equally, regardless of their original range or magnitude.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3834528-0a9c-4137-8fef-bee0906221f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.74047963, 0.47700227, 0.26661783, 0.95274493],\n",
       "       [0.82002809, 0.50632787, 0.28801233, 0.9499124 ],\n",
       "       [0.8148006 , 0.49875588, 0.29542603, 0.95116727],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Import MinMaxScaler from sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Drop the 'Label' column from the DataFrame to get features\n",
    "features = df.drop(['Label'], axis='columns')\n",
    "\n",
    "# Initialize MinMaxScaler\n",
    "features_scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the features using MinMaxScaler\n",
    "features = features_scaler.fit_transform(features)\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacc8bb6-d683-48c9-bbad-f46371a60cf7",
   "metadata": {},
   "source": [
    "## Data Normalization\n",
    "\n",
    "A copy of the DataFrame `df` is created for normalization. The specified feature columns (`contrast`, `homogenity`, `energy`, `correlation`) are updated with the scaled features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c001f390-4e33-4e6e-9807-255fc7a68f8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contrast</th>\n",
       "      <th>homogeneity</th>\n",
       "      <th>energy</th>\n",
       "      <th>correlation</th>\n",
       "      <th>Label</th>\n",
       "      <th>homogenity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.740480</td>\n",
       "      <td>0.422618</td>\n",
       "      <td>0.266618</td>\n",
       "      <td>0.952745</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.477002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.820028</td>\n",
       "      <td>0.448600</td>\n",
       "      <td>0.288012</td>\n",
       "      <td>0.949912</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.506328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.814801</td>\n",
       "      <td>0.441892</td>\n",
       "      <td>0.295426</td>\n",
       "      <td>0.951167</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.498756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.800124</td>\n",
       "      <td>0.346679</td>\n",
       "      <td>0.223333</td>\n",
       "      <td>0.943737</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.391291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.783606</td>\n",
       "      <td>0.386176</td>\n",
       "      <td>0.271969</td>\n",
       "      <td>0.950147</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.435871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3998</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      contrast  homogeneity    energy  correlation  Label  homogenity\n",
       "0     0.740480     0.422618  0.266618     0.952745    0.0    0.477002\n",
       "1     0.820028     0.448600  0.288012     0.949912    0.0    0.506328\n",
       "2     0.814801     0.441892  0.295426     0.951167    0.0    0.498756\n",
       "3     0.800124     0.346679  0.223333     0.943737    0.0    0.391291\n",
       "4     0.783606     0.386176  0.271969     0.950147    0.0    0.435871\n",
       "...        ...          ...       ...          ...    ...         ...\n",
       "3995  0.000000     0.000000  0.000000     0.000000    0.0    0.000000\n",
       "3996  0.000000     0.000000  0.000000     0.000000    0.0    0.000000\n",
       "3997  0.000000     0.000000  0.000000     0.000000    0.0    0.000000\n",
       "3998  0.000000     0.000000  0.000000     0.000000    0.0    0.000000\n",
       "3999  0.000000     0.000000  0.000000     0.000000    0.0    0.000000\n",
       "\n",
       "[4000 rows x 6 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a copy of the original DataFrame for normalization\n",
    "data_normalization = df.copy()\n",
    "\n",
    "# Update specified feature columns with scaled features\n",
    "data_normalization[['contrast', 'homogenity', 'energy', 'correlation']] = features\n",
    "data_normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b5f32c-27e9-4f28-a2a2-5a2cbb0f95aa",
   "metadata": {},
   "source": [
    "## Generating Descriptive Statistics for Normalized Data\n",
    "\n",
    "The `data_normalization.describe()` method provides a summary of statistics for the normalized DataFrame `data_normalization`. This method is useful for examining the distribution, central tendencies, and variability of the normalized feature values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "60094ec4-ee83-4510-b382-e5627290bf66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contrast</th>\n",
       "      <th>homogeneity</th>\n",
       "      <th>energy</th>\n",
       "      <th>correlation</th>\n",
       "      <th>Label</th>\n",
       "      <th>homogenity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4000.000000</td>\n",
       "      <td>4000.000000</td>\n",
       "      <td>4000.000000</td>\n",
       "      <td>4000.000000</td>\n",
       "      <td>4000.000000</td>\n",
       "      <td>4000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.161565</td>\n",
       "      <td>0.154877</td>\n",
       "      <td>0.124561</td>\n",
       "      <td>0.407317</td>\n",
       "      <td>0.133500</td>\n",
       "      <td>0.174807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.270128</td>\n",
       "      <td>0.185359</td>\n",
       "      <td>0.161129</td>\n",
       "      <td>0.460474</td>\n",
       "      <td>0.340157</td>\n",
       "      <td>0.209211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.186308</td>\n",
       "      <td>0.308051</td>\n",
       "      <td>0.240381</td>\n",
       "      <td>0.922138</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.347692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.885988</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          contrast  homogeneity       energy  correlation        Label  \\\n",
       "count  4000.000000  4000.000000  4000.000000  4000.000000  4000.000000   \n",
       "mean      0.161565     0.154877     0.124561     0.407317     0.133500   \n",
       "std       0.270128     0.185359     0.161129     0.460474     0.340157   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.186308     0.308051     0.240381     0.922138     0.000000   \n",
       "max       1.000000     0.885988     1.000000     1.000000     1.000000   \n",
       "\n",
       "        homogenity  \n",
       "count  4000.000000  \n",
       "mean      0.174807  \n",
       "std       0.209211  \n",
       "min       0.000000  \n",
       "25%       0.000000  \n",
       "50%       0.000000  \n",
       "75%       0.347692  \n",
       "max       1.000000  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate descriptive statistics for the normalized DataFrame\n",
    "data_normalization.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcdf86f-a485-46e0-828d-ad378adfa267",
   "metadata": {},
   "source": [
    "## Splitting Features and Labels\n",
    "\n",
    "In the context of preparing data for machine learning, it's essential to separate the features from the target labels. \n",
    "\n",
    "- **Features (`x`)**: The variable `x` is created by dropping the 'Label' column from the `data_normalization` DataFrame. This includes all the normalized features (e.g., contrast, homogenity, energy, correlation) that the model will use to make predictions.\n",
    "\n",
    "- **Target Labels (`y`)**: The variable `y` is set to the 'Label' column of the `data_normalization` DataFrame. This column represents the target variable that the model will learn to predict based on the features.\n",
    "\n",
    "The 'Label' column is selected as the target variable because it represents the outcome or category that we want the machine learning model to predict. \n",
    "\n",
    "Choosing the 'Label' column is crucial for guiding the learning process and measuring the effectiveness of the model.\n",
    "This separation allows the machine learning model to learn the relationship between the features and the corresponding labels during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4fd3ae42-c564-452a-aa00-a733a0ce3f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a feature set by dropping the 'Label' column from the DataFrame\n",
    "x = data_normalization.drop(['Label'], axis='columns')\n",
    "\n",
    "# Assign the 'Label' column to the target variable\n",
    "y = data_normalization.Label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d0edaa-da28-4239-b39c-ca2fb47d6f36",
   "metadata": {},
   "source": [
    "## Linear Regression Model with Classification\n",
    "\n",
    "#### 1. Data Splitting\n",
    "The dataset is split into training (80%) and testing (20%) using `train_test_split`. \n",
    "- `X_train`, `X_test`: Features.\n",
    "- `y_train`, `y_test`: Target labels.\n",
    "\n",
    "#### 2. Model Training\n",
    "A `LinearRegression` model is created and trained on `X_train` and `y_train`.\n",
    "\n",
    "#### 3. Predictions\n",
    "The model predicts continuous outputs on `X_test`.\n",
    "\n",
    "#### 4. Regression Metrics\n",
    "- **MSE (Mean Squared Error)**: Measures the average squared error.\n",
    "- **RMSE (Root Mean Squared Error)**: Provides the error in the same units as the target.\n",
    "\n",
    "#### 5. Classification\n",
    "The continuous outputs are converted into binary classes using a threshold of 0.5.\n",
    "\n",
    "#### 6. Classification Metrics\n",
    "- **Accuracy**: Overall correctness of predictions.\n",
    "- **Precision**: Ratio of true positives to total predicted positives.\n",
    "- **Recall**: Ratio of true positives to actual positives.\n",
    "- **F1-Score**: Balance between precision and recall.\n",
    "\n",
    "This gives insight into the model's performance for both regression and classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f474c276-1ad0-48ec-8120-85a0baa1ece7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 0.10047023227282176\n",
      "Root Mean Squared Error (RMSE): 0.31697039652437853\n",
      "Accuracy: 0.85125\n",
      "Precision (weighted): 0.7765192388122125\n",
      "Recall (weighted): 0.85125\n",
      "F1-Score (weighted): 0.7863460625059371\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set (continuous output)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# For regression tasks:\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f'Mean Squared Error (MSE): {mse}')\n",
    "print(f'Root Mean Squared Error (RMSE): {rmse}')\n",
    "\n",
    "# For classification (convert regression output to binary class labels):\n",
    "threshold = 0.5\n",
    "y_pred_class = np.where(y_pred >= threshold, 1, 0)\n",
    "\n",
    "# Calculate classification metrics with zero_division=1 to suppress warnings\n",
    "accuracy = accuracy_score(y_test, y_pred_class)\n",
    "precision = precision_score(y_test, y_pred_class, average='weighted', zero_division=1)\n",
    "recall = recall_score(y_test, y_pred_class, average='weighted', zero_division=1)\n",
    "f1 = f1_score(y_test, y_pred_class, average='weighted', zero_division=1)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision (weighted): {precision}')\n",
    "print(f'Recall (weighted): {recall}')\n",
    "print(f'F1-Score (weighted): {f1}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2e2f9f-a971-472d-9ec0-d6c028bf16de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7313f7cd-51d4-4639-801b-d0c8b921cdaa",
   "metadata": {},
   "source": [
    "# HOG (Histogram Of Gradients) Feature Extraction\n",
    "\n",
    "\n",
    "HOG is a feature descriptor used in computer vision for object detection and recognition. It captures the following key features:\n",
    "\n",
    "1. **Gradient Magnitude**: Represents the strength of edges in the image.\n",
    "\n",
    "2. **Gradient Orientation**: Indicates the direction of edges, providing shape information.\n",
    "\n",
    "3. **Histograms of Gradients**: For each cell (e.g., 8x8 pixels), a histogram is created to represent the distribution of gradient orientations.\n",
    "\n",
    "4. **Normalized Histograms**: Histograms are normalized to improve robustness against lighting variations.\n",
    "\n",
    "5. **Spatial Arrangement**: Captures the spatial distribution of features within blocks of cells, aiding in pattern recognition.\n",
    "\n",
    "6. **Multi-Scale Representation**: HOG can be computed at different scales to detect objects of various sizes.\n",
    "\n",
    "7. **Concatenated Feature Vector**: The final output is a high-dimensional vector combining all normalized histograms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "209d8530-545a-4470-b9b3-248ed9dd0e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Extracting Features\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "from sklearn.decomposition import PCA\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "def extract_features(image_path):\n",
    "    # Create a HOG descriptor object\n",
    "    hog = cv2.HOGDescriptor()\n",
    "    \n",
    "    # Read the image from the given path\n",
    "    img = cv2.imread(image_path)\n",
    "    \n",
    "    # Resize the image to (64, 128) pixels for HOG feature extraction\n",
    "    resized = cv2.resize(img, (64, 128), interpolation=cv2.INTER_AREA)\n",
    "    \n",
    "    # Compute the HOG features for the resized image\n",
    "    h = hog.compute(resized)\n",
    "    \n",
    "    # Return the transposed HOG features for consistent shape\n",
    "    return h.T\n",
    "\n",
    "def apply_pca(features, num_components):\n",
    "    # Apply PCA to reduce the dimensionality of the features\n",
    "    pca = PCA(n_components=num_components)\n",
    "    \n",
    "    # Fit and transform the features using PCA\n",
    "    reduced_features = pca.fit_transform(features)\n",
    "    \n",
    "    # Return the reduced feature set\n",
    "    return reduced_features\n",
    "\n",
    "def write_to_csv(features, categories, filename):\n",
    "    # Prepare data to write to a CSV file\n",
    "    csv_data = []\n",
    "    for id, line in enumerate(features):\n",
    "        new_img = line.tolist()  # Convert numpy array to list\n",
    "        new_img.insert(0, categories[id])  # Insert the corresponding category label\n",
    "        csv_data.append(new_img)  # Append the data for this image\n",
    "    \n",
    "    # Write the data to the specified CSV file\n",
    "    with open(filename, 'w', newline='') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerows(csv_data)  # Write all rows to the CSV file\n",
    "\n",
    "def main():\n",
    "    num_fea = 100  # Number of PCA components to retain\n",
    "    category = []  # List to store category labels\n",
    "    hog_array = []  # List to store HOG features\n",
    "\n",
    "    # Iterate through folders in the training dataset directory\n",
    "    for folder in os.listdir(\"split_combination/train\"):\n",
    "        # Skip specific files that are not directories\n",
    "        if folder == \"my_model2.h5\" or folder == \"feature_hog\":\n",
    "            continue\n",
    "            \n",
    "        # Iterate through files in the current category folder\n",
    "        for filename in os.listdir(os.path.join(\"split_combination/train\", folder)):\n",
    "            # Check if the file is an image (png or jpg)\n",
    "            if filename[-3:] in [\"png\", \"jpg\"]:\n",
    "                image_path = os.path.join(\"split_combination/train\", folder, filename)\n",
    "                \n",
    "                # Extract HOG features from the image\n",
    "                hog_image = extract_features(image_path)\n",
    "                \n",
    "                # Append the extracted HOG features to the list\n",
    "                hog_array.append(hog_image)\n",
    "                \n",
    "                # Append the corresponding category label (1 for Glaucoma, 0 for Normal)\n",
    "                if folder == \"Glaucoma\":\n",
    "                    category.append(1)\n",
    "                elif folder == \"Normal\":\n",
    "                    category.append(0)\n",
    "\n",
    "    # Convert the list of HOG features to a numpy array\n",
    "    hog_array_np = np.array(hog_array)\n",
    "    \n",
    "    # Reshape the HOG array to 2D for PCA processing\n",
    "    reshaped_hog_array = np.reshape(hog_array_np, (hog_array_np.shape[0], hog_array_np.shape[1]))\n",
    "    \n",
    "    # Apply PCA to reduce the dimensionality of the HOG features\n",
    "    reduced_features = apply_pca(reshaped_hog_array, num_fea)\n",
    "    \n",
    "    # Write the reduced features and corresponding categories to a CSV file\n",
    "    write_to_csv(reduced_features, category, 'split_combination/extracted_features.csv')\n",
    "\n",
    "    print(\"Done Extracting Features\")  # Indicate completion of feature extraction\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()  # Run the main function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0cae01-d015-4d12-91db-579311d5333e",
   "metadata": {},
   "source": [
    "## Feature Extraction Analysis with Pandas\n",
    "\n",
    "This Python script utilizes the Pandas library to analyze the features extracted from images of glaucoma and normal classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "54340b9f-f837-496c-be4f-1fcefd026419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               0             1             2             3             4    \\\n",
      "count  1230.000000  1.230000e+03  1.230000e+03  1.230000e+03  1.230000e+03   \n",
      "mean      0.304065  2.665456e-07 -7.624276e-08  1.544744e-07  3.441660e-08   \n",
      "std       0.460197  1.101019e+00  9.826329e-01  9.073421e-01  7.144700e-01   \n",
      "min       0.000000 -2.427125e+00 -2.103099e+00 -2.507950e+00 -1.195083e+00   \n",
      "25%       0.000000 -8.274099e-01 -9.001857e-01 -6.544465e-01 -4.344090e-01   \n",
      "50%       0.000000 -7.740784e-02  7.693342e-02 -1.641440e-01 -1.253433e-01   \n",
      "75%       1.000000  8.024688e-01  8.716102e-01  5.125992e-01  2.137835e-01   \n",
      "max       1.000000  2.749097e+00  2.661562e+00  3.445665e+00  3.145810e+00   \n",
      "\n",
      "                5             6             7             8             9    \\\n",
      "count  1.230000e+03  1.230000e+03  1.230000e+03  1.230000e+03  1.230000e+03   \n",
      "mean   1.167220e-08  5.359532e-08 -1.609111e-07 -2.230586e-08  6.449627e-08   \n",
      "std    5.986914e-01  5.337544e-01  5.130715e-01  4.910246e-01  4.755001e-01   \n",
      "min   -1.629097e+00 -1.640897e+00 -1.543833e+00 -1.554404e+00 -1.284404e+00   \n",
      "25%   -4.623832e-01 -3.575672e-01 -3.343390e-01 -3.286769e-01 -3.177556e-01   \n",
      "50%    1.832493e-02  8.214106e-03  2.028946e-02  6.413298e-03  7.882952e-03   \n",
      "75%    4.339441e-01  3.786743e-01  3.430167e-01  3.095491e-01  3.260374e-01   \n",
      "max    1.727319e+00  1.650860e+00  1.720115e+00  2.014483e+00  1.591260e+00   \n",
      "\n",
      "       ...           91            92            93            94   \\\n",
      "count  ...  1.230000e+03  1.230000e+03  1.230000e+03  1.230000e+03   \n",
      "mean   ... -7.889102e-09 -5.586267e-08  4.412334e-08 -2.804904e-08   \n",
      "std    ...  1.733190e-01  1.723677e-01  1.710671e-01  1.706763e-01   \n",
      "min    ... -5.679371e-01 -5.528458e-01 -5.877786e-01 -5.613735e-01   \n",
      "25%    ... -1.113728e-01 -1.099696e-01 -1.162959e-01 -1.122719e-01   \n",
      "50%    ...  2.672824e-03  1.642961e-03 -3.080999e-04  1.424936e-03   \n",
      "75%    ...  1.178374e-01  1.122929e-01  1.125431e-01  1.098929e-01   \n",
      "max    ...  8.107989e-01  7.237207e-01  6.242214e-01  6.156516e-01   \n",
      "\n",
      "                95            96            97            98            99   \\\n",
      "count  1.230000e+03  1.230000e+03  1.230000e+03  1.230000e+03  1.230000e+03   \n",
      "mean   8.341835e-08  5.444681e-08  1.401098e-07  1.776380e-07 -1.473581e-08   \n",
      "std    1.689951e-01  1.684470e-01  1.669889e-01  1.662338e-01  1.654627e-01   \n",
      "min   -5.426915e-01 -5.718205e-01 -5.234498e-01 -4.452907e-01 -5.575099e-01   \n",
      "25%   -1.087818e-01 -1.097812e-01 -1.163073e-01 -1.153377e-01 -1.111828e-01   \n",
      "50%   -9.344276e-04 -1.890902e-04  1.598976e-03  2.288213e-03 -6.565650e-04   \n",
      "75%    1.091657e-01  1.075177e-01  1.127810e-01  1.051547e-01  1.209930e-01   \n",
      "max    5.975688e-01  6.472837e-01  5.618373e-01  6.230114e-01  6.217538e-01   \n",
      "\n",
      "                100  \n",
      "count  1.230000e+03  \n",
      "mean   7.748547e-08  \n",
      "std    1.643102e-01  \n",
      "min   -5.320598e-01  \n",
      "25%   -1.139050e-01  \n",
      "50%   -1.465681e-03  \n",
      "75%    1.042002e-01  \n",
      "max    6.523110e-01  \n",
      "\n",
      "[8 rows x 101 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"split_combination/extracted_features.csv\", header=None)\n",
    "\n",
    "# Print descriptive statistics\n",
    "print(data.describe())\n",
    "\n",
    "# Optional: specify column names\n",
    "#data = pd.read_csv(\"/Users/admin/images1/extracted_features.csv\", header=None, names=[\"feature1\", \"feature2\", ...])\n",
    "\n",
    "# Optional: handle missing values\n",
    "# data.fillna(data.mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609ae2fa-a5cb-4745-b159-5a8ae37b2a1f",
   "metadata": {},
   "source": [
    "## Data Preparation for Model Training\n",
    "\n",
    "This focuses on separating the feature set (X) from the target variable (Y) after loading the dataset into a Pandas DataFrame.\n",
    "\n",
    "\n",
    "- \\( X \\) represents the feature set, which is created by selecting all rows from the DataFrame (data) and the columns indexed from 1 to 100. This implies that the first column (index 0) is excluded, as it typically contains the labels or target variable.\n",
    "\n",
    "- \\( Y \\) represents the target variable, which is the first column of the DataFrame. This column usually contains the labels that the model will learn to predict based on the features in \\( X \\)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e284ab07-0dbe-46bc-a25e-55ba8b78062b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:,range(1,101)]\n",
    "Y = data.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7044cf8a-6fe6-4cb8-917d-553572451c9e",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation\n",
    "\n",
    "This code demonstrates training a Linear Regression model and evaluating its performance. It begins by loading the dataset, assuming the feature set \\( X \\) and target variable \\( Y \\) are already defined. The dataset is split into training (70%) and testing (30%) sets using `train_test_split`.\n",
    "\n",
    "A `LinearRegression` model is then initialized and trained on the training set. Predictions are made on the test set, resulting in continuous output values, which are converted to binary class labels using a threshold of 0.5.\n",
    "\n",
    "The code calculates various metrics to evaluate the model's performance, including accuracy, precision, recall, and F1 score for classification, along with mean squared error (MSE) and R-squared (R²) for regression. Finally, it prints these metrics to assess how well the model predicts the target variable based on the input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "da16d214-c4f7-48cc-b0df-50efbbcc3923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.68\n",
      "Precision: 0.47\n",
      "Recall: 0.21\n",
      "F1 Score: 0.29\n",
      "Mean Squared Error: 0.22\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, mean_squared_error, r2_score\n",
    "\n",
    "# Load the dataset\n",
    "# Assuming X and Y are already defined as in your original code\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize and train the Linear Regression model\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, Y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "Y_pred_continuous = regressor.predict(X_test)\n",
    "\n",
    "# Convert continuous predictions to binary class labels (using a threshold, e.g., 0.5)\n",
    "threshold = 0.5\n",
    "Y_pred_class = [1 if pred >= threshold else 0 for pred in Y_pred_continuous]\n",
    "\n",
    "# Calculate classification metrics\n",
    "accuracy = accuracy_score(Y_test, Y_pred_class)\n",
    "precision = precision_score(Y_test, Y_pred_class)\n",
    "recall = recall_score(Y_test, Y_pred_class)\n",
    "f1 = f1_score(Y_test, Y_pred_class)\n",
    "\n",
    "# Calculate regression metrics\n",
    "mse = mean_squared_error(Y_test, Y_pred_continuous)\n",
    "r2 = r2_score(Y_test, Y_pred_continuous)\n",
    "\n",
    "# Print the classification results\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "\n",
    "# Print the regression metrics\n",
    "print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fcde6a-025a-41b2-a626-f24b425d58bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
